{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building an AI Agent using Agentic AI"
      ],
      "metadata": {
        "id": "5wipGyFQsPpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# define stock symbol and time period\n",
        "symbol = \"AAPL\"\n",
        "start_date = \"2020-01-01\"\n",
        "end_date = \"2025-02-14\"\n",
        "\n",
        "# download historical data\n",
        "data = yf.download(symbol, start=start_date, end=end_date)"
      ],
      "metadata": {
        "id": "U95lLoJcsR1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering\n",
        "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
        "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
        "data['Returns'] = data['Close'].pct_change()"
      ],
      "metadata": {
        "id": "0y5WDBxXsddz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop NaN values and reset index\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "HgtaOx7Wsfk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define action space\n",
        "ACTIONS = {0: \"HOLD\", 1: \"BUY\", 2: \"SELL\"}"
      ],
      "metadata": {
        "id": "_BYt0_vBsgyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get state function\n",
        "def get_state(data, index):\n",
        "    return np.array([\n",
        "        float(data.loc[index, 'Close']),\n",
        "        float(data.loc[index, 'SMA_5']),\n",
        "        float(data.loc[index, 'SMA_20']),\n",
        "        float(data.loc[index, 'Returns'])\n",
        "    ])"
      ],
      "metadata": {
        "id": "g95_Tib6sjgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trading environment\n",
        "class TradingEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.initial_balance = 10000\n",
        "        self.balance = self.initial_balance\n",
        "        self.holdings = 0\n",
        "        self.index = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.holdings = 0\n",
        "        self.index = 0\n",
        "        return get_state(self.data, self.index)\n",
        "\n",
        "    def step(self, action):\n",
        "        price = float(self.data.loc[self.index, 'Close'])\n",
        "        reward = 0\n",
        "\n",
        "        if action == 1 and self.balance >= price:  # BUY\n",
        "            self.holdings = self.balance // price\n",
        "            self.balance -= self.holdings * price\n",
        "        elif action == 2 and self.holdings > 0:  # SELL\n",
        "            self.balance += self.holdings * price\n",
        "            self.holdings = 0\n",
        "\n",
        "        self.index += 1\n",
        "        done = self.index >= len(self.data) - 1\n",
        "\n",
        "        if done:\n",
        "            reward = self.balance - self.initial_balance\n",
        "\n",
        "        next_state = get_state(self.data, self.index) if not done else None\n",
        "        return next_state, reward, done, {}"
      ],
      "metadata": {
        "id": "RWRq6LMJslK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deep q-network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "kXFGbQh_sqvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = DQN(state_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(list(ACTIONS.keys()))\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "                target += self.gamma * torch.max(self.model(next_state_tensor)).item()\n",
        "\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            target_tensor = self.model(state_tensor).clone().detach()\n",
        "            target_tensor[0][action] = target\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(state_tensor)\n",
        "            loss = self.criterion(output, target_tensor)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "OPMKGVGGsrtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the agent\n",
        "env = TradingEnvironment(data)\n",
        "agent = DQNAgent(state_size=4, action_size=3)\n",
        "batch_size = 32\n",
        "episodes = 500\n",
        "total_rewards = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    agent.replay(batch_size)\n",
        "    total_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "id": "0o_kmd-ls0LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a fresh environment instance for testing\n",
        "test_env = TradingEnvironment(data)\n",
        "state = test_env.reset()\n",
        "done = False\n",
        "\n",
        "# simulate a trading session using the trained agent\n",
        "while not done:\n",
        "    # always choose the best action (exploitation)\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, _ = test_env.step(action)\n",
        "    state = next_state if next_state is not None else state\n",
        "\n",
        "final_balance = test_env.balance\n",
        "profit = final_balance - test_env.initial_balance\n",
        "print(f\"Final Balance after testing: ${final_balance:.2f}\")\n",
        "print(f\"Total Profit: ${profit:.2f}\")"
      ],
      "metadata": {
        "id": "_yjVKRD7s21X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}