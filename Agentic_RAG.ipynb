{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/agentic-rag-tutorial"
      ],
      "metadata": {
        "id": "Kjs4ts1pv1p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-groq faiss-cpu crewai serper pypdf2 python-dotenv setuptools sentence-transformers huggingface distutils`"
      ],
      "metadata": {
        "id": "YfVlnaorv3LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os from dotenv\n",
        "import load_dotenv from langchain.vectorstores\n",
        "import FAISS from langchain.document_loaders\n",
        "import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from crewai_tools import SerperDevTool\n",
        "from crewai import Agent, Task, Crew, LLM\n",
        "\n",
        "load_dotenv()\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
        "GEMINI=os.getenv(\"GEMINI\")"
      ],
      "metadata": {
        "id": "JhafcmGSv4-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-specdec\",\n",
        "    temperature=0,\n",
        "    max_tokens=500,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "crew_llm = LLM(\n",
        "    model=\"gemini/gemini-1.5-flash\",\n",
        "    api_key=GEMINI,\n",
        "    max_tokens=500,\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "hOAJ-XKBv6Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_local_knowledge(query, context):\n",
        "    \"\"\"Router function to determine if we can answer from local knowledge\"\"\"\n",
        "    prompt = '''Role: Question-Answering Assistant\n",
        "Task: Determine whether the system can answer the user's question based on the provided text.\n",
        "Instructions:\n",
        "    - Analyze the text and identify if it contains the necessary information to answer the user's question.\n",
        "    - Provide a clear and concise response indicating whether the system can answer the question or not.\n",
        "    - Your response should include only a single word. Nothing else, no other text, information, header/footer.\n",
        "Output Format:\n",
        "    - Answer: Yes/No\n",
        "Study the below examples and based on that, respond to the last question.\n",
        "Examples:\n",
        "    Input:\n",
        "        Text: The capital of France is Paris.\n",
        "        User Question: What is the capital of France?\n",
        "    Expected Output:\n",
        "        Answer: Yes\n",
        "    Input:\n",
        "        Text: The population of the United States is over 330 million.\n",
        "        User Question: What is the population of China?\n",
        "    Expected Output:\n",
        "        Answer: No\n",
        "    Input:\n",
        "        User Question: {query}\n",
        "        Text: {text}\n",
        "'''\n",
        "    formatted_prompt = prompt.format(text=context, query=query)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    return response.content.strip().lower() == \"yes\""
      ],
      "metadata": {
        "id": "rzj6UnOqv8d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_web_scraping_agent():\n",
        "    \"\"\"Setup the web scraping agent and related components\"\"\"\n",
        "    search_tool = SerperDevTool()  # Tool for performing web searches\n",
        "    scrape_website = ScrapeWebsiteTool()  # Tool for extracting data from websites\n",
        "\n",
        "    # Define the web search agent\n",
        "    web_search_agent = Agent(\n",
        "        role=\"Expert Web Search Agent\",\n",
        "        goal=\"Identify and retrieve relevant web data for user queries\",\n",
        "        backstory=\"An expert in identifying valuable web sources for the user's needs\",\n",
        "        allow_delegation=False,\n",
        "        verbose=True,\n",
        "        llm=crew_llm\n",
        "    )\n",
        "\n",
        "    # Define the web scraping agent\n",
        "    web_scraper_agent = Agent(\n",
        "        role=\"Expert Web Scraper Agent\",\n",
        "        goal=\"Extract and analyze content from specific web pages identified by the search agent\",\n",
        "        backstory=\"A highly skilled web scraper, capable of analyzing and summarizing website content accurately\",\n",
        "        allow_delegation=False,\n",
        "        verbose=True,\n",
        "        llm=crew_llm\n",
        "    )\n",
        "\n",
        "    # Define the web search task\n",
        "    search_task = Task(\n",
        "        description=(\n",
        "            \"Identify the most relevant web page or article for the topic: '{topic}'. \"\n",
        "            \"Use all available tools to search for and provide a link to a web page \"\n",
        "            \"that contains valuable information about the topic. Keep your response concise.\"\n",
        "        ),\n",
        "        expected_output=(\n",
        "            \"A concise summary of the most relevant web page or article for '{topic}', \"\n",
        "            \"including the link to the source and key points from the content.\"\n",
        "        ),\n",
        "        tools=[search_tool],\n",
        "        agent=web_search_agent,\n",
        "    )\n",
        "\n",
        "    # Define the web scraping task\n",
        "    scraping_task = Task(\n",
        "        description=(\n",
        "            \"Extract and analyze data from the given web page or website. Focus on the key sections \"\n",
        "            \"that provide insights into the topic: '{topic}'. Use all available tools to retrieve the content, \"\n",
        "            \"and summarize the key findings in a concise manner.\"\n",
        "        ),\n",
        "        expected_output=(\n",
        "            \"A detailed summary of the content from the given web page or website, highlighting the key insights \"\n",
        "            \"and explaining their relevance to the topic: '{topic}'. Ensure clarity and conciseness.\"\n",
        "        ),\n",
        "        tools=[scrape_website],\n",
        "        agent=web_scraper_agent,\n",
        "    )\n",
        "\n",
        "    # Define the crew to manage agents and tasks\n",
        "    crew = Crew(\n",
        "        agents=[web_search_agent, web_scraper_agent],\n",
        "        tasks=[search_task, scraping_task],\n",
        "        verbose=1,\n",
        "        memory=False,\n",
        "    )\n",
        "    return crew\n",
        "\n",
        "def get_web_content(query):\n",
        "    \"\"\"Get content from web scraping\"\"\"\n",
        "    crew = setup_web_scraping_agent()\n",
        "    result = crew.kickoff(inputs={\"topic\": query})\n",
        "    return result.raw"
      ],
      "metadata": {
        "id": "MA-TlaoTv-kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_vector_db(pdf_path):\n",
        "    \"\"\"Setup vector database from PDF\"\"\"\n",
        "    # Load and chunk PDF\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=50\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Create vector database\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "    )\n",
        "    vector_db = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    return vector_db\n",
        "\n",
        "def get_local_content(vector_db, query):\n",
        "    \"\"\"Get content from vector database\"\"\"\n",
        "    docs = vector_db.similarity_search(query, k=5)\n",
        "    return \" \".join([doc.page_content for doc in docs])"
      ],
      "metadata": {
        "id": "0-z66_syv_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_answer(context, query):\n",
        "    \"\"\"Generate final answer using LLM\"\"\"\n",
        "    messages = [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Use the provided context to answer the query accurately.\",\n",
        "        ),\n",
        "        (\"system\", f\"Context: {context}\"),\n",
        "        (\"human\", query),\n",
        "    ]\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "def process_query(query, vector_db, local_context):\n",
        "    \"\"\"Main function to process user query\"\"\"\n",
        "    print(f\"Processing query: {query}\")\n",
        "\n",
        "    # Step 1: Check if we can answer from local knowledge\n",
        "    can_answer_locally = check_local_knowledge(query, local_context)\n",
        "    print(f\"Can answer locally: {can_answer_locally}\")\n",
        "\n",
        "    # Step 2: Get context either from local DB or web\n",
        "    if can_answer_locally:\n",
        "        context = get_local_content(vector_db, query)\n",
        "        print(\"Retrieved context from local documents\")\n",
        "    else:\n",
        "        context = get_web_content(query)\n",
        "        print(\"Retrieved context from web scraping\")\n",
        "\n",
        "    # Step 3: Generate final answer\n",
        "    answer = generate_final_answer(context, query)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "0dZxyVuZwCB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Setup\n",
        "    pdf_path = \"genai-principles.pdf\"\n",
        "\n",
        "    # Initialize vector database\n",
        "    print(\"Setting up vector database...\")\n",
        "    vector_db = setup_vector_db(pdf_path)\n",
        "\n",
        "    # Get initial context from PDF for routing\n",
        "    local_context = get_local_content(vector_db, \"\")\n",
        "\n",
        "    # Example usage\n",
        "    query = \"What is Agentic RAG?\"\n",
        "    result = process_query(query, vector_db, local_context)\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(result)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CoRoSn4ywDdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agentic RAG is a technique for building Large Language Model (LLM) powered applications that incorporates AI agents. It is an extension of the traditional Retrieval-Augmented Generation (RAG) approach, which uses an external knowledge source to provide the LLM with relevant context and reduce hallucinations.\n",
        "In traditional RAG pipelines, the retrieval component is typically composed of an embedding model and a vector database, and the generative component is an LLM. At inference time, the user query is used to run a similarity search over the indexed documents to retrieve the most similar documents to the query and provide the LLM with additional context.\n",
        "Agentic RAG, on the other hand, introduces AI agents that are designed to interact with the user query and provide additional context to the LLM. These agents can be thought of as \"virtual assistants\" that help the LLM to better understand the user's intent and retrieve relevant documents.\n",
        "\n",
        "The key components of Agentic RAG include:\n",
        "1. AI agents: These are the virtual assistants that interact with the user query and provide additional context to the LLM.\n",
        "2. Retrieval component: This is the component that retrieves the most similar documents to the user query.\n",
        "3. Generative component: This is the component that uses the retrieved documents to generate the final output.\n",
        "\n",
        "Agentic RAG has several benefits, including:\n",
        "1. Improved accuracy: By providing additional context to the LLM, Agentic RAG can improve the accuracy of the generated output.\n",
        "2. Enhanced user experience: Agentic RAG can help to reduce the complexity of the user interface and provide a more natural and intuitive experience.\n",
        "3. Increased flexibility: Agentic RAG can be easily extended to support new use cases and applications.\n",
        "\n",
        "However, Agentic RAG also has some limitations, including:\n",
        "1. Increased complexity: Agentic RAG requires additional components and infrastructure, which can increase the complexity of the system.\n",
        "2. Higher computational requirements: Agentic RAG requires more computational resources to handle the additional complexity of the AI agents and the retrieval component.\n",
        "3. Training requirements: Agentic RAG requires more data and training to learn the behaviour of the AI agents and the retrieval component."
      ],
      "metadata": {
        "id": "ogjf49d9wExm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVIEA_H9wGgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}