{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Image Caption Recommendation System"
      ],
      "metadata": {
        "id": "YOiMyprqlN8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This repository demonstrates how to use\n",
        "\n",
        "*   Build an AI Image Caption Recommendation System using a retrieval-based approach. The system leverages the CLIP (Contrastive Language–Image Pre-training) model to understand both image and text content. The goal is to recommend the most relevant captions for a given image from a predefined list of captions. This approach is particularly useful for social media platforms where captions need to be engaging and contextually relevant.\n",
        "\n"
      ],
      "metadata": {
        "id": "nk8MMhj0ldz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Packages\n",
        "Before running the script, ensure you have the necessary libraries installed. You can install them using pip:\n",
        "\n",
        "torch: PyTorch library for deep learning.\n",
        "transformers: Library for pre-trained models like CLIP.\n",
        "Pillow: Library for image processing.\n",
        "scikit-learn: Library for cosine similarity calculation."
      ],
      "metadata": {
        "id": "ZUqyw8qFsiwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers pillow scikit-learn"
      ],
      "metadata": {
        "id": "vDzJ4DejmPYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "M1AJqLINlHEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function opens the image using PIL, converts it to RGB format (important for consistency), and then uses the CLIPProcessor to transform the image into a format suitable for the CLIP model. The processor handles resizing, normalization, and other necessary transformations. The output will be a PyTorch tensor ready for CLIP."
      ],
      "metadata": {
        "id": "PeX0rLNTsymY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    return inputs, processor"
      ],
      "metadata": {
        "id": "jykSk0hPktEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Image Embeddings\n",
        "\n",
        "This function loads the pre-trained CLIP model. The crucial step is model.get_image_features(**inputs), which passes the preprocessed image tensor to the CLIP model and extracts a high-dimensional feature vector representing the image’s visual content. torch.no_grad() is used to prevent gradient calculations during inference, saving memory and speeding up the process.\n"
      ],
      "metadata": {
        "id": "HklUO0hys46f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_embeddings(inputs):\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(**inputs)\n",
        "    return image_features, model"
      ],
      "metadata": {
        "id": "NfQqALr4tKQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Match Captions\n",
        "\n",
        "This function takes the image features and a list of candidate captions as input. It processes the captions using the same CLIPProcessor (now for text) to get text embeddings. It then calculates the cosine similarity between the image embedding and each text embedding. Cosine similarity measures the angle between two vectors; a value closer to 1 indicates higher similarity. The function will return the captions ranked by similarity and their corresponding similarity scores."
      ],
      "metadata": {
        "id": "3DfPToDqtNfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_captions(image_features, captions, clip_model, processor):\n",
        "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.get_text_features(**text_inputs)\n",
        "\n",
        "    image_features = image_features.detach().cpu().numpy()\n",
        "    text_features = text_features.detach().cpu().numpy()\n",
        "\n",
        "    similarities = cosine_similarity(image_features, text_features)\n",
        "\n",
        "    best_indices = similarities.argsort(axis=1)[0][::-1]\n",
        "    best_captions = [captions[i] for i in best_indices]\n",
        "\n",
        "    return best_captions, similarities[0][best_indices].tolist()"
      ],
      "metadata": {
        "id": "YdpuDEXNtP3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function\n",
        "\n",
        "This function ties together the preprocessing, feature extraction, and matching processes. It takes an image path and a list of candidate captions, processes the image, gets its features, and then matches these features against the captions. The result will be a list of best-fit captions with their similarity scores."
      ],
      "metadata": {
        "id": "mIqWTs6NtTFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_captioning(image_path, candidate_captions):\n",
        "    inputs, processor = load_and_preprocess_image(image_path)\n",
        "    image_features, clip_model = generate_image_embeddings(inputs)\n",
        "\n",
        "    best_captions, similarities = match_captions(image_features, candidate_captions, clip_model, processor)\n",
        "    return best_captions, similarities"
      ],
      "metadata": {
        "id": "dXqutF3wtRcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Captions"
      ],
      "metadata": {
        "id": "pnbT6-Irta7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_captions = [\n",
        "    \"Trees, Travel and Tea!\",\n",
        "    \"A refreshing beverage.\",\n",
        "    \"A moment of indulgence.\",\n",
        "    \"The perfect thirst quencher.\",\n",
        "    \"Your daily dose of delight.\",\n",
        "    \"Taste the tradition.\",\n",
        "    \"Savor the flavor.\",\n",
        "    \"Refresh and rejuvenate.\",\n",
        "    \"Unwind and enjoy.\",\n",
        "    \"The taste of home.\",\n",
        "    \"A treat for your senses.\",\n",
        "    \"A taste of adventure.\",\n",
        "    \"A moment of bliss.\",\n",
        "    \"Your travel companion.\",\n",
        "    \"Fuel for your journey.\",\n",
        "    \"The essence of nature.\",\n",
        "    \"The warmth of comfort.\",\n",
        "    \"A sip of happiness.\",\n",
        "    \"Pure indulgence.\",\n",
        "    \"Quench your thirst, ignite your spirit.\",\n",
        "    \"Awaken your senses, embrace the moment.\",\n",
        "    \"The taste of faraway lands.\",\n",
        "    \"A taste of home, wherever you are.\",\n",
        "    \"Your daily dose of delight.\",\n",
        "    \"Your moment of serenity.\",\n",
        "    \"The perfect pick-me-up.\",\n",
        "    \"The perfect way to unwind.\",\n",
        "    \"Taste the difference.\",\n",
        "    \"Experience the difference.\",\n",
        "    \"A refreshing escape.\",\n",
        "    \"A delightful escape.\",\n",
        "    \"The taste of tradition, the spirit of adventure.\",\n",
        "    \"The warmth of home, the joy of discovery.\",\n",
        "    \"Your passport to flavor.\",\n",
        "    \"Your ticket to tranquility.\",\n",
        "    \"Sip, savor, and explore.\",\n",
        "    \"Indulge, relax, and rejuvenate.\",\n",
        "    \"The taste of wanderlust.\",\n",
        "    \"The comfort of home.\",\n",
        "    \"A journey for your taste buds.\",\n",
        "    \"A haven for your senses.\",\n",
        "    \"Your refreshing companion.\",\n",
        "    \"Your delightful escape.\",\n",
        "    \"Taste the world, one sip at a time.\",\n",
        "    \"Embrace the moment, one cup at a time.\",\n",
        "    \"The essence of exploration.\",\n",
        "    \"The comfort of connection.\",\n",
        "    \"Quench your thirst for adventure.\",\n",
        "    \"Savor the moment of peace.\",\n",
        "    \"The taste of discovery.\",\n",
        "    \"The warmth of belonging.\",\n",
        "    \"Your travel companion, your daily delight.\",\n",
        "    \"Your moment of peace, your daily indulgence.\",\n",
        "    \"The spirit of exploration, the comfort of home.\",\n",
        "    \"The joy of discovery, the warmth of connection.\",\n",
        "    \"Sip, savor, and set off on an adventure.\",\n",
        "    \"Indulge, relax, and find your peace.\",\n",
        "    \"A delightful beverage.\",\n",
        "    \"A moment of relaxation.\",\n",
        "    \"The perfect way to start your day.\",\n",
        "    \"The perfect way to end your day.\",\n",
        "    \"A treat for yourself.\",\n",
        "    \"Something to savor.\",\n",
        "    \"A moment of calm.\",\n",
        "    \"A taste of something special.\",\n",
        "    \"A refreshing pick-me-up.\",\n",
        "    \"A comforting drink.\",\n",
        "    \"A taste of adventure.\",\n",
        "    \"A moment of peace.\",\n",
        "    \"A small indulgence.\",\n",
        "    \"A daily ritual.\",\n",
        "    \"A way to connect with others.\",\n",
        "    \"A way to connect with yourself.\",\n",
        "    \"A taste of home.\",\n",
        "    \"A taste of something new.\",\n",
        "    \"A moment to enjoy.\",\n",
        "    \"A moment to remember.\"\n",
        "]"
      ],
      "metadata": {
        "id": "IfJ2axCVtYeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the System"
      ],
      "metadata": {
        "id": "CxMBw65utg0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "best_captions, similarities = image_captioning(\"/content/aman.png\", candidate_captions)\n",
        "\n",
        "# Get the top 5 results\n",
        "top_n = min(5, len(best_captions))\n",
        "top_best_captions = best_captions[:top_n]\n",
        "top_similarities = similarities[:top_n]\n",
        "\n",
        "print(\"Top 5 Best Captions:\")\n",
        "for i, (caption, similarity) in enumerate(zip(top_best_captions, top_similarities)):\n",
        "    print(f\"{i+1}. {caption} (Similarity: {similarity:.4f})\")"
      ],
      "metadata": {
        "id": "5vRcCB1GtdfQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}